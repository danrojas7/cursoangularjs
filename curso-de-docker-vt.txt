
En 2016 fue la aparición de los contenedores digitales dentro del desarrollo de software, principalmente de docker, lo que ofrede docker es un nuevo modo de empaquetar, ejecutar y distribuir aplicaciones

Empaquetar en un único bloque todas las dependencias que necesita para ejecutarse, desde los binarios, librería, configuración y runtime todo lo que se pueda instalar en un servidor, el bloque recibe el nombre de contenedor docker, una unidad estándar para el desarrollo y despliegue de aplicaciones. Esto garantiza que el software se va a comportar del mismo modo sin importar en qué ambiente se ejecute

Por ejemplo se pueden ejecutar contenedores con Node.js y con Java, que son dos tecnologías que no tienen en común entre sí, sin tener necesidad de tener instalado ninguno de los dos en la áquina host.

No se trata ni de un lenguaje de programación, ni de un framework, es una herrmienta que ayuda a solventar problemas comunes con la instalación, distribución y gestión de software. Permite a los programadores y a los DevOps construir, distribuir y ejecutar su código en cualquier sitio.

Esto se puede confundir con un motor de virtualización pero no es así:
La máquina virtual tradicional que representa la virtualización a nivel de hardware, no es más que un completo sistema operativo que se ejecuta sobre un sistema operativo host, la principal ventaja es que se pueden ejecutar muchas máquinas virtuales con sistemas operativos virtuales totalmente diferentes en un único host. Las máquinas virtuales están totalmente aisladas, de ahí a que sea muy seguras, pero esto tiene un coste, ya que tienen que contener todo lo que un sistema operativo necesita, lo que los convierte un gran consumidor de recursos, lo que limita la cantidad de máquinas virtuales que se ejecutan en un único host.

Por su parte, el software docker se ejecuta en un entorno aislado llamado contenedor docker, un contenedor docker no es una máquina virtual, las imágenes docker se ejecutan dentro del mismo kernel del sistema operativo, un contenedor puede tener sus propios archivos del sistema y sus propios archivos de configuración y variables de entorno, son totalmente autosuficientes. Pero como los contenedores se ejecutan dentro del mismo kernel, consumen muy pocos recursos del sistema, son muy ligeros, y no cuentan con los problemas que conlleva la vitualización clásica. Esto es lo que permite que una aplicación incluída dentro un contenedor docker se acerque mucho al rendimiento nativo, el booteo de una aplicación docker es usualmente muy rápido, debido a la baja sobrecarga de los contenedores. Docker por tanto es diferente a los motores de vitualización, no está totalmente asilado con relación a éstos, pero tienen rendimiento mucho más alto

Resumen de los beneficio de los contenedores:
- Gran rendimiento
- Creación y destrucción rápida
- Reducido tiempo de Provisión

Docker solo comparte el kernel, pero reutiliza las capas imagen, lo que permite un despliegue rápido, una migración sencilla y unos tiempos de arranque bastante ágiles.
Otro beneficio de docker, es que permite generar builds portables, y extremadamente fáciles de distribuir. La aplicación se ejecuta dentro de su propio contenedor, por lo que no es necesaria su instalación tradicional. La ventaja clave de una imagen docker es que incluye todas las dependencias que una aplicación necesita para ejecutarse, la ausencia de instalación de dependencias es una gran ventaja, elimina los problemas como los conflictos entre software y librerías, o problemas de compatibilidades de drivers. El uso de contenedores docker acelera la integración contínua, se acaron los ciclos interminables de build, test y deploy. Los contenedores docker nos asegura que las aplicaciones se va a ejecutar del mismo modo en desarrollo, en test y en entornos de producción.

Una de las grandes características de docker es la portabilidad, por lo que los contenedores docker son portables, por lo que se pueden ejecutar desde cualquier sitio, por ejemplo en una máquina local, un servidor cercano o lejano, o en la cloud ya sea pública o privada.

Los mayores proveedores de computing cloud ya soportan docker. Podemos desarrollar software , si preocuparnos por la plataforma en la que se va a ejecutar más adelante. Realmente se trata de una solución para escribir el código una vez, y ejecutarlo en cualquier sitio.

Docker por tanto, nos permite contar con un servidor ligero que no necesita de gestión de la configuración y en el que manejamos nuestras aplicaciones simplemente desplegando y redesplegando contenedores al servidor, como estos contenedores son muy ligeros, se ejecutan de un modo muy rápido. Como un punto de inicio, se pueden descargar imagenes preconstruidas desde la página de docker hub. Se trata de un repositorio de imágenes preparadas para usar, hay elecciones de todo tipo servidores web, platformas runtime, bases de datos, servicios de mensajería entre otros. Es como una auténtica mina de oro de software que podemos usar por libre para tener una base o un fundamiento para iniciar nuestros proyectos.

El efecto de la inmutabilidad de las imágenes docker es el resultado en el modo en el que son creadas, docker hace uso de un archivo de configuración llamado dockerfile, este archivo dockerfile contiene todas las instrucciones de configuración sobre como crear la imagen, los componentes obligatorios, librerías, directorios compartidos, configuración de red y demás. Una imagen puede ser muy básica, que solo puede tener los fundamentos del sistema operativo o algo que es más común, una completa pila de tecnologías preconstruidas que están preparadas para lanzarse.

Pero desde luego docker, no solo es un procesador dockerfile y un motor runtime, es un paquete completo con una amplia selección de herramientas y APIs que son útiles para el trabajo diario del desarrollador, y lo primero de todo son los docker toolbox, se trata de un installer para Windos y Mac, que nos permite fácilmente instalar y configurar un entorno docker nuestra propia máquina.

Las herramientas que contiene docker:
- Docker engine
- Compose
- Machine
- Kitematic - Entorno de desarrollo de escritorio para usar docker tanto en Windows como en Mac


Una de las grandes ventajas de docker es crear instancias remotamente, mediante la docker machine

Actualizar el software del sistema operativo
$ sudo yum update -y
$ zypper update

Instalar docker
$ sudo yum install -y docker
$ zypper install docker

Iniciar el servicio de docker
$ sudo service docker start

Configurar el servicio para que se inicie con el sistema
sudo chkconfig docker on

Añadir el usuario al grupo docker
$ sudo usermod -a -G docker name-user

Iniciar una nueva imagen de docker
$ docker run hello-world

Para detener un contenedor:
$ docker stop

Listar todos los contenedores docker en ejecución:
$ docker ps
Listar todos los contenedores docker:
$ docker ps -a
Listar el tamaño de los contenedores
$ docker ps -s

En las últimas versiones de docker:
Listar todos los contenedores en ejecución:
$ docker container ls

Para traer una imagen de dockerhub, se utiliza el siguiente comando
$ docker pull kitematic/hello-world-nginx

Para eliminar un contenedor docker, se utiliza el siguiente comando:
$ docker rm id
Donde Id es el identificador del contenedor

Para comprobar las imágenes que se encuentran en docker, se utiliza el siguiente comando:
$ docker images
Para eliminar imágenes, se puede mediante el comando siguiente:
$ docker rmi nombredelaimagen

para eliminar una imagen que esté siendo usada por el contenedor, se debe pasar el modificador -f
$ docker rmi -f kitematic/hello-world-nginx:latest

Se pueden usar los contenedores para generar nuestras propias imágenes personalizadas, desde la terminal podemos salvar el contenedor que queramos como una imagen personalizada, para ello se requiere la id del contenedor o el nombre, y se utiliza con el siguiente comando:
$ docker commit 33e54776e5a5
El comando utilizado tiene un comportamiento similar a como se utiliza en git

También se puede hacer commit estableciendo el nombre personalizado de la imagen:
$ docker commit goofy_kepler danrojas7/mynginx

Al listar las imágenes con el siguiente comando, se pueden observar las imágenes:
$ docker images


Finalmente, para correr el contenedor a partir de la imagen, personalizada, se utiliza el comando siguiente:
$ docker run --name test-nginx -d danrojas7/mynginx

La imágenes también pueden crearse mediante el archivo dockerfile

Como utilizar dockerhub ara subir imagenes
- Crear una cuenta en dockerhub
- Desde la interfaz web de dockerhub se permite crear un repositorio, pero se va a hacer mediante la línea de comandos:
  Antes de hacer push a la imagen personalizada, se va a cambiar el tag si se requiere, para ello utilizamos el siguiente comando:
$ docker tag danrojas7/mi-ingx danrojas7/nginx-subido
  Ahora se debe loguearse en la terminal a dockerhub, para ello se dene ingresar los siguientes comandos:
$ docker login
Ingresar el nombre de usuario  y la contraseña con la que se haya creado la cuenta en dockerhub
$ docker push danrojas7/nginx-subido
Si volvemos al navegador web, se puede observar la imagen recién subida

Las redes en los contenedores
Cuando creamos un contenedor, no es muy útil si no podemos conectarlo, en alguno de los contenedores configurados, se ha utilizado la opción p que permite cunfigurar nuestros puertos, lo que hacemos es conectar a la dirección IP de nuestra máquina host para acceder a nuestros contenedores. Tenemos también direcciones IP que están asignadas a cada contenedor, estas direcciones IP llegan desde la red docker instala cuando se instala la docker machine. Docker machine nos provee con redes prediseñadas pero adicionalmente provee un par de drivers de red diferentes para nuestro uso: los drivers bridge y overlay, el overlay sólo se utiliza cuando se tenga múltiples host, y para la mayoría de los casos la red bridge es todo lo que necesitamos, básicamente lo que hace es crear una red al vuelo en la máquina host, en cualquier momento que creemos un contenedor, se añade a la red bridge por defecto.

https://docs.docker.com/network/

Para listar las redes que tengamos activas, se puede hacer mediante el siguiente comando:
$ docker network ls
Asegurarse que se encuentre un contenedor en ejecución, por medio de kitematic
Mediante el comando
$ docker network inspect bridge
Este comando, muestra las propiedades de la red bridge, cualquier contenedor que añadamos, automáticamente se va a listar en las propiedades de la red junto con la dirección IP. Uno de los grandes beneficios del uso de docker, es que nos permite construir aplicaciones a partir de microservicios, lo que significa es que podemos conectar una base de datos a un frontend para construir una aplicacion web por ejemplo, en la mayoría de los casos es necesaria una dirección IP para ello.

También podemos crear nuestra propia red, con el comando network create, de la siguiente manera:
$ docker network create mi-red

Si listamos las redes, vemos como nos aparece la red que acabamos de crear
$ docker network ls

Si inspeccionamos la red mediante el uso del siguiente comando:
$ docker network inspect mi-red

Podemos ver que no tiene ningún contenedor ningún contenedor adicionado.

Para conectar un contenedor existente a la red recién creada, en primer lugar se debe desconectar el contenedor de la red brigde a la que se agrega por defecto cuando el contenedor es creado, esto se hace mediante el comando siguiente:
$ docker network disconnect bridge nginx-compartido

Y a continuación se conecta mediante el siguiente comando:
$ docker network connect mi-red focused_minsky

Para realizar puente entre el contenedor incluido en la red bridge, se ejecuta el siguiente comando:
$ docker run --name mi-apache -p 2244:80 -v //d/drojas/apache:/usr/local/apache2/htdocs/ -d httpd
--name mi-apache    # nombre del contenedor
-p 2244:80          # se le indica el puerto, 2244 de la máquina host, puenteado al puerto 80 del contenedor
-v //d/drojas/apache:/usr/local/apache2/htdocs/
                    # se le indica la configuración del volumen, el volumen del disco local //d/drojas/apache, vinculado al
                    # volumen remoto /usr/local/apache2/htdocs/, que es e que utiliza apache para servir las páginas web que se
                    # encuentren adentro de este directorios
-d httpd            # indica la imagen en la que se va a basar el contenedor
                    
La configuración del mapeo de los volúmenes, lo incluye el contenedor en su dockerfile

Para instalar ghost, no requiere sino solamente la imagen oficial
Para instalar wordpress, como requiere una solución multistack, en lo que se refiere a la imagen bitnami/wordpress, lo que se debe hacer es ingresar a la documentación de wordpress para docker, e instalarlo según esas instrucciones, creando el archivo de configutación, y con el siguiente comando, descarga las imágenes y crea los contenedores docker:
$ docker-compose up -d


Crear una imagen docker de una aplicación Flask de python

Crear una imagen docker con la aplicación que se encuentra en el repositorio git
https://github.com/0utKast/flask-app.git

todas la imágenes que samos, están basadas en una imagen base, como la imagen es de python, debemos construir una imagen que esté basada en python.
Se utiliza alpine para crear la imagen


Un dockerfile es un archivo de texto que contiene una lista de comandos que el daemon de docker va a invocar cuado cree una imagen, El dockerfile contiene la información que necesita docker para saber cómo ejecutar esa aplicación, se trata de un modo sencillo de automatizar el proceso de creación de imágenes y lo mejor es que los comandos que se escriben en el dockerfile, es el equivalente a escribir comando linux.
https://docs.docker.com/engine/reference/builder/

Con el siguiente comando, se va a crear una imagen docker a partir del contenido del directorio de flask con aplicación python
docker build -t danrojas7/miprimerapp .

Por último, se crea el contenedor docker a partir de la imagen
docker run --name miprimerimagen -p 8888:5000 -d danrojas7/miprimerapp

Crear un entorno docker en windows 10
docker, el sistema que automatiza el despliegue de aplicaciones, dentro de contenedores de software, ha permitido el crecimiento acelerado de microservicios, junto con kubernetes, el sistema para el manejo de esas aplicaciones, ya convertido en un ecosistema cada vez más importante para los desarrolladores, pero la instalación, implementación e integración de estas dos herramientas podría hacerse un poco complicada especialmente en windows. Se podía porque ahora ahora en windows 10 contamos con una aplicación nativa que proporciona un entorno de desarrollo facil de usar, para crear, enviar y ejecutar aplicaciones dockerizadas y totalmente integradas con kubernetes, para empezar indicar que este sistema solo está accesible para las versiones pro y enterprise de windows, también para la versión de educación. Porque usa virtualización Hyper-V, que no están disponibles en la versión Home, para la cual existe la llamada docker toolbox, pero con la limitación para integrarse con Kubernetes.

La aplicación Docker Community Edition For Windows, se instala como cualquier programa windows.
Se comprueba la versión mediante el siguiente comando:
 $ docker --version
Ingresar a settings
La versión de docker para windows, incluye el soporte para kubernetes, por lo que se puede probar la implementación de sus cargas de trabajo docker en kubernetes.
El comando del cliente de kubernetes es kubectl, y está incluído y configurado para conectarse al servidor kubernetes local y apunta a docker for desktop
Se puede habilitar kubernetes, aunque se puede elegir la herramienta Swarm de orquestación incluida en la versión community de docker

Al ir a la shell, se pueden ejecutar comandos del cliente de kubernetes
Muestra la versión del clúster y la versión del servidor
$ kubectl version

Diagnósicos al clúster
$ kubectl get componentstatuses

Listar los nodos en el clúster
$ kubectl get nodes

Mostrar información detallada de cualquiera de los nodos
$ kubectl describe nodes

Con esto ya contamos con un completo entorno de trabajo para el desarrollo de microservicios sobre contenedores y con un sistema de orquestación integrados. Solo queda añadir que docker nos permite utilizar tanto contenedores linux como windows, por defecto, docker viene configurado para trabajar con contenedores linux, pero esto se puede cambiar en cualquier momento en el ícono de notificación de docker.

Mostrar información acerca de los deamons del clúster y los puertos que están utilizando
$ kubectl cluster-info

Intalar minikubes para usar kubernetes local en windows
Los sistemas de contenedores tienen que ejecutarse en servidores, y la tendencia creciente es hacerlo en servicios en la nube. Pero siempre, en cualquier caso, vamos a tener también que ejecutarlo en local. Para trabajar en windows, existe minicube, una herramienta que hace que sea facil ejecutar kubernetes localmente. Minikube ejecuta un cluster de kubernetes en nuestro ordenador, con lo que ya contamos con una infraestructura de inicio sin necesidad de configuración.

Instalar inikube en windows
- Instalar virtualbox
- Instalar kubectl
  CLI, cliente de línea de comandos de kubernetes
  Se puede instalar mediante el manejador de paquetes de windows, que es chocolately, que es un completo sistema de administración de paquetes, que hace similar a lo que hace apt get  o yum en sistemas linux o en unix, lo hace chocolately en windows
  
  Una vez instalado chocolately, se procede a instalar kubectl, con el siguiente comando en el powershell o en el cmd
  $ choco install kubernetes-cli
  
  Para verificar la versión, y validar que se haya instalado de forma correcta, se hace mediante el siguiente comando:
  $ kubectl version
  
- Instalar el minikube
  https://github.com/kubernetes/minikube

  Una vez instalado, se puede comprobar que está funcionando.
  
Borrar la versión que está instalada ahora
$ minikube delete

Con este comando inicia el kubernetes local, inicia la máquina virtual, lleva a cabo todo el proceso de configuración, 
$ minikube start

Para comprobar que tanto kubectl como minikube, estan funcionando de modo correcto, ejecutamos el siguiente comando:
$ kubectl get nodes

Con esto ya contamos con una completa infraestructura para el desarrollo y orquestación de contenedores mediante el uso de docker y kubernetes mediante el uso de minikube


